{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "608e64bc-10a3-4daa-ab89-6f35e0dc984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24c76740-b3fd-4b63-b93e-5e1236b24c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preparation\n",
    "text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\"\n",
    "\n",
    "# Clean and tokenize the text\n",
    "text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "words = text.split()\n",
    "\n",
    "# Create vocabulary mappings\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id) + 1  # Add 1 for padding\n",
    "\n",
    "# Convert words to token IDs\n",
    "tokenized_text = [word2id[word] for word in words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b550d010-fd61-4b31-9e21-d567b2001244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'of': 2, 'a': 3, 'processes': 4, 'we': 5, 'are': 6, 'to': 7, 'computational': 8, 'process': 9, 'abstract': 10, 'called': 11, 'about': 12, 'study': 13, 'idea': 14, 'beings': 15, 'that': 16, 'inhabit': 17, 'computers': 18, 'as': 19, 'they': 20, 'evolve': 21, 'manipulate': 22, 'other': 23, 'things': 24, 'data': 25, 'evolution': 26, 'is': 27, 'directed': 28, 'by': 29, 'pattern': 30, 'rules': 31, 'program': 32, 'people': 33, 'create': 34, 'programs': 35, 'direct': 36, 'in': 37, 'effect': 38, 'conjure': 39, 'spirits': 40, 'computer': 41, 'with': 42, 'our': 43, 'spells': 44}\n",
      "\n",
      "\n",
      "{1: 'the', 2: 'of', 3: 'a', 4: 'processes', 5: 'we', 6: 'are', 7: 'to', 8: 'computational', 9: 'process', 10: 'abstract', 11: 'called', 12: 'about', 13: 'study', 14: 'idea', 15: 'beings', 16: 'that', 17: 'inhabit', 18: 'computers', 19: 'as', 20: 'they', 21: 'evolve', 22: 'manipulate', 23: 'other', 24: 'things', 25: 'data', 26: 'evolution', 27: 'is', 28: 'directed', 29: 'by', 30: 'pattern', 31: 'rules', 32: 'program', 33: 'people', 34: 'create', 35: 'programs', 36: 'direct', 37: 'in', 38: 'effect', 39: 'conjure', 40: 'spirits', 41: 'computer', 42: 'with', 43: 'our', 44: 'spells'}\n"
     ]
    }
   ],
   "source": [
    "print(word2id)\n",
    "print(\"\\n\")\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b08f35b5-7e63-4bed-bd9d-92ddcd5ed5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  6,  7, 13],\n",
       "       [ 6, 12, 13,  1],\n",
       "       [12,  7,  1, 14],\n",
       "       [ 7, 13, 14,  2],\n",
       "       [13,  1,  2,  3],\n",
       "       [ 1, 14,  3,  8],\n",
       "       [14,  2,  8,  9],\n",
       "       [ 2,  3,  9,  8],\n",
       "       [ 3,  8,  8,  4],\n",
       "       [ 8,  9,  4,  6],\n",
       "       [ 9,  8,  6, 10],\n",
       "       [ 8,  4, 10, 15],\n",
       "       [ 4,  6, 15, 16],\n",
       "       [ 6, 10, 16, 17],\n",
       "       [10, 15, 17, 18],\n",
       "       [15, 16, 18, 19],\n",
       "       [16, 17, 19, 20],\n",
       "       [17, 18, 20, 21],\n",
       "       [18, 19, 21,  4],\n",
       "       [19, 20,  4, 22],\n",
       "       [20, 21, 22, 23],\n",
       "       [21,  4, 23, 10],\n",
       "       [ 4, 22, 10, 24],\n",
       "       [22, 23, 24, 11],\n",
       "       [23, 10, 11, 25],\n",
       "       [10, 24, 25,  1],\n",
       "       [24, 11,  1, 26],\n",
       "       [11, 25, 26,  2],\n",
       "       [25,  1,  2,  3],\n",
       "       [ 1, 26,  3,  9],\n",
       "       [26,  2,  9, 27],\n",
       "       [ 2,  3, 27, 28],\n",
       "       [ 3,  9, 28, 29],\n",
       "       [ 9, 27, 29,  3],\n",
       "       [27, 28,  3, 30],\n",
       "       [28, 29, 30,  2],\n",
       "       [29,  3,  2, 31],\n",
       "       [ 3, 30, 31, 11],\n",
       "       [30,  2, 11,  3],\n",
       "       [ 2, 31,  3, 32],\n",
       "       [31, 11, 32, 33],\n",
       "       [11,  3, 33, 34],\n",
       "       [ 3, 32, 34, 35],\n",
       "       [32, 33, 35,  7],\n",
       "       [33, 34,  7, 36],\n",
       "       [34, 35, 36,  4],\n",
       "       [35,  7,  4, 37],\n",
       "       [ 7, 36, 37, 38],\n",
       "       [36,  4, 38,  5],\n",
       "       [ 4, 37,  5, 39],\n",
       "       [37, 38, 39,  1],\n",
       "       [38,  5,  1, 40],\n",
       "       [ 5, 39, 40,  2],\n",
       "       [39,  1,  2,  1],\n",
       "       [ 1, 40,  1, 41],\n",
       "       [40,  2, 41, 42],\n",
       "       [ 2,  1, 42, 43],\n",
       "       [ 1, 41, 43, 44]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Generate Training Data\n",
    "window_size = 2  # Context window size\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Create context-target pairs\n",
    "for i in range(window_size, len(tokenized_text) - window_size):\n",
    "    context = tokenized_text[i - window_size:i] + tokenized_text[i + 1:i + 1 + window_size]\n",
    "    target = tokenized_text[i]\n",
    "    X_train.append(context)\n",
    "    y_train.append(target)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = to_categorical(y_train, num_classes=vocab_size)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0d975ba-270e-435d-b02c-dc0130dd4f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.0115 - loss: 3.8079     \n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0449 - loss: 3.8059 \n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0449 - loss: 3.8042 \n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0668 - loss: 3.8025 \n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.0553 - loss: 3.8006\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0460 - loss: 3.7988      \n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.1002 - loss: 3.7970  \n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.1325 - loss: 3.7951  \n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.1566 - loss: 3.7926  \n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.1785 - loss: 3.7916\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1681 - loss: 3.7903\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1889 - loss: 3.7874 \n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2098 - loss: 3.7853 \n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2213 - loss: 3.7842 \n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2328 - loss: 3.7816\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2223 - loss: 3.7803 \n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2130 - loss: 3.7785  \n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.2223 - loss: 3.7762\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2328 - loss: 3.7737 \n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.1900 - loss: 3.7725  \n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2317 - loss: 3.7691  \n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.1994 - loss: 3.7671  \n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.1785 - loss: 3.7652  \n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1994 - loss: 3.7628\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.1994 - loss: 3.7606 \n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.1889 - loss: 3.7593  \n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.1785 - loss: 3.7583  \n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1889 - loss: 3.7553 \n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.1681 - loss: 3.7525  \n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.1889 - loss: 3.7500\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1462 - loss: 3.7496 \n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.1994 - loss: 3.7444  \n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.1889 - loss: 3.7423 \n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1889 - loss: 3.7399 \n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1889 - loss: 3.7376 \n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1785 - loss: 3.7363 \n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1994 - loss: 3.7320 \n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.2306 - loss: 3.7237\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2108 - loss: 3.7250  \n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2108 - loss: 3.7214 \n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.2108 - loss: 3.7208\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2108 - loss: 3.7148  \n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2317 - loss: 3.7105  \n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2223 - loss: 3.7117 \n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 712us/step - accuracy: 0.2536 - loss: 3.7043\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2223 - loss: 3.7034  \n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.2223 - loss: 3.6987  \n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.2328 - loss: 3.6986\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1911 - loss: 3.6966 \n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2119 - loss: 3.6940 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17716925490>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Define and Train the Model\n",
    "embedding_dim = 10\n",
    "\n",
    "# CBOW Model using Keras Sequential API\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=window_size * 2),\n",
    "    Lambda(lambda x: K.mean(x, axis=1)),  # Average embeddings for context\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "098269b0-d0cb-421b-abfb-e6197f09beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predicted word for the context ['computational', 'process', 'abstract', 'beings']: a\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Prediction\n",
    "# Function to predict the target word for a given context\n",
    "def predict_word(context_words):\n",
    "    context_ids = [word2id[word] for word in context_words if word in word2id]\n",
    "    context_ids = np.array(context_ids).reshape(1, -1)  # Reshape for prediction\n",
    "    prediction = model.predict(context_ids)\n",
    "    predicted_word_id = np.argmax(prediction)\n",
    "    return id2word[predicted_word_id]\n",
    "\n",
    "# Testing with a context\n",
    "test_context = ['computational', 'process', 'abstract', 'beings']\n",
    "predicted_word = predict_word(test_context)\n",
    "print(f\"Predicted word for the context {test_context}: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d092fb3c-3f91-4d48-bef9-fe6a6b27429d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b9a7b8-af6b-402f-ae43-74ffd8921474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11276c8-6682-49a9-9957-1f7057a5af89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
